{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AI-Driven Lunar Soil Composition Analysis - Data Acquisition & Training\n",
                "\n",
                "## Overview\n",
                "This notebook handles:\n",
                "1. **Data Acquisition**: Downloading Chang'e 3 PCAM and TCAM data.\n",
                "2. **Data Preprocessing**: Creating a custom PyTorch Dataset.\n",
                "3. **Model Training**: Training a ResNet-18 model for terrain classification.\n",
                "\n",
                "## Setup\n",
                "Ensure you are running this in an environment with internet access (e.g., Google Colab)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import pandas as pd\n",
                "from urllib.parse import urljoin\n",
                "from PIL import Image\n",
                "from io import BytesIO\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms, models\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "\n",
                "# specific directories for data\n",
                "DATA_DIR = '../data'\n",
                "PCAM_DIR = os.path.join(DATA_DIR, 'pcam')\n",
                "TCAM_DIR = os.path.join(DATA_DIR, 'tcam')\n",
                "\n",
                "os.makedirs(PCAM_DIR, exist_ok=True)\n",
                "os.makedirs(TCAM_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Data directories created: {PCAM_DIR}, {TCAM_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Downloading Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_image_links(url):\n",
                "    \"\"\"Fetches image URLs from the Planetary Society index pages.\"\"\"\n",
                "    try:\n",
                "        response = requests.get(url)\n",
                "        response.raise_for_status()\n",
                "        soup = BeautifulSoup(response.content, 'html.parser')\n",
                "        links = []\n",
                "        for a in soup.find_all('a', href=True):\n",
                "            href = a['href']\n",
                "            if href.lower().endswith(('.png', '.jpg')):\n",
                "                full_url = urljoin(url, href)\n",
                "                links.append(full_url)\n",
                "        return links\n",
                "    except Exception as e:\n",
                "        print(f\"Error fetching links from {url}: {e}\")\n",
                "        return []\n",
                "\n",
                "def download_images(image_urls, save_dir, limit=20):\n",
                "    \"\"\"Downloads a limited number of images to the save directory.\"\"\"\n",
                "    count = 0\n",
                "    for url in image_urls:\n",
                "        if count >= limit:\n",
                "            break\n",
                "        filename = os.path.basename(url)\n",
                "        save_path = os.path.join(save_dir, filename)\n",
                "        if os.path.exists(save_path):\n",
                "            # print(f\"Skipping {filename} (already exists)\")\n",
                "            count += 1\n",
                "            continue\n",
                "        try:\n",
                "            img_data = requests.get(url).content\n",
                "            with open(save_path, 'wb') as f:\n",
                "                f.write(img_data)\n",
                "            print(f\"Downloaded {filename}\")\n",
                "            count += 1\n",
                "        except Exception as e:\n",
                "            print(f\"Failed to download {url}: {e}\")\n",
                "    print(f\"Finished downloading {count} images to {save_dir}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# URLs from the Planetary Society article\n",
                "PCAM_INDEX_URL = 'http://planetary.s3.amazonaws.com/data/change3/pcam.html'\n",
                "TCAM_INDEX_URL = 'http://planetary.s3.amazonaws.com/data/change3/tcam.html'\n",
                "\n",
                "print(\"Fetching PCAM links...\")\n",
                "pcam_links = get_image_links(PCAM_INDEX_URL)\n",
                "print(f\"Found {len(pcam_links)} PCAM images.\")\n",
                "\n",
                "print(\"Fetching TCAM links...\")\n",
                "tcam_links = get_image_links(TCAM_INDEX_URL)\n",
                "print(f\"Found {len(tcam_links)} TCAM images.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download a sample set (e.g., 50 images from each)\n",
                "if pcam_links:\n",
                "    print(\"Downloading sample PCAM images...\")\n",
                "    download_images(pcam_links, PCAM_DIR, limit=50)\n",
                "\n",
                "if tcam_links:\n",
                "    print(\"Downloading sample TCAM images...\")\n",
                "    download_images(tcam_links, TCAM_DIR, limit=50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Loading & Preprocessing\n",
                "We define a custom Dataset to load the images we just downloaded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LunarDataset(Dataset):\n",
                "    def __init__(self, root_dirs, transform=None):\n",
                "        self.image_paths = []\n",
                "        self.transform = transform\n",
                "        \n",
                "        for d in root_dirs:\n",
                "            if os.path.exists(d):\n",
                "                for f in os.listdir(d):\n",
                "                    if f.lower().endswith(('.png', '.jpg')):\n",
                "                        self.image_paths.append(os.path.join(d, f))\n",
                "                        \n",
                "        print(f\"Total images loaded: {len(self.image_paths)}\")\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.image_paths)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_path = self.image_paths[idx]\n",
                "        # Convert to RGB as some pngs might be RGBA\n",
                "        image = Image.open(img_path).convert('RGB')\n",
                "        \n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "            \n",
                "        # DUMMY LABEL GENERATION\n",
                "        # In a real scenario, you would look up the label from a CSV based on filename\n",
                "        # Classes: 0=Regolith, 1=Crater, 2=Boulder\n",
                "        label = torch.randint(0, 3, (1,)).item() \n",
                "        \n",
                "        return image, label\n",
                "\n",
                "# Transforms for ResNet (224x224)\n",
                "data_transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "dataset = LunarDataset([PCAM_DIR, TCAM_DIR], transform=data_transform)\n",
                "\n",
                "# Train/Val Split\n",
                "train_size = int(0.8 * len(dataset))\n",
                "val_size = len(dataset) - train_size\n",
                "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
                "\n",
                "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Definition\n",
                "Using ResNet-18 for transfer learning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_model(num_classes=3, device='cpu'):\n",
                "    # Load pretrained resnet\n",
                "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
                "    num_ftrs = model.fc.in_features\n",
                "    # Modify last layer\n",
                "    model.fc = nn.Sequential(\n",
                "        nn.Dropout(0.5),\n",
                "        nn.Linear(num_ftrs, num_classes)\n",
                "    )\n",
                "    model.to(device)\n",
                "    return model\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "model = get_model(device=device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_model(model, train_loader, val_loader, num_epochs=5):\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "    \n",
                "    for epoch in range(num_epochs):\n",
                "        model.train()\n",
                "        running_loss = 0.0\n",
                "        \n",
                "        for images, labels in train_loader:\n",
                "            images = images.to(device)\n",
                "            labels = labels.to(device)\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            running_loss += loss.item()\n",
                "            \n",
                "        avg_loss = running_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
                "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
                "        \n",
                "        # Validation\n",
                "        model.eval()\n",
                "        correct = 0\n",
                "        total = 0\n",
                "        with torch.no_grad():\n",
                "            for images, labels in val_loader:\n",
                "                images = images.to(device)\n",
                "                labels = labels.to(device)\n",
                "                outputs = model(images)\n",
                "                _, predicted = torch.max(outputs.data, 1)\n",
                "                total += labels.size(0)\n",
                "                correct += (predicted == labels).sum().item()\n",
                "        \n",
                "        acc = 100 * correct / total if total > 0 else 0\n",
                "        print(f\"   Val Accuracy: {acc:.2f}%\")\n",
                "\n",
                "# Start Training\n",
                "if len(dataset) > 0:\n",
                "    train_model(model, train_loader, val_loader, num_epochs=5)\n",
                "else:\n",
                "    print(\"No data found. Please check download step.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save & Download Model\n",
                "Since we are running in Colab, the file is saved on the remote server. We need to explicitly download it to your local machine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs('../models', exist_ok=True)\n",
                "save_path = '../models/lunar_terrain_model.pth'\n",
                "\n",
                "if len(dataset) > 0:\n",
                "    torch.save(model.state_dict(), save_path)\n",
                "    print(f\"Model saved remotely to {save_path}\")\n",
                "    \n",
                "    # ---- NEW: DOWNLOAD LOGIC ----\n",
                "    print(\"Attempting to download file to your local machine...\")\n",
                "    try:\n",
                "        from google.colab import files\n",
                "        files.download(save_path)\n",
                "        print(\"Download should start in your browser.\")\n",
                "    except ImportError:\n",
                "        print(\"Could not import google.colab. If you are running locally, the file is already at:\", os.path.abspath(save_path))\n",
                "    except Exception as e:\n",
                "        print(f\"Download Error: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}